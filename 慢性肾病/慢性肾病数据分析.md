

# 慢性肾病数据分析



------

## 背景分析

慢性肾病是指肾脏清除血液中代谢废物的能力逐渐下降（数月到数年）。慢性肾病（`chronic kidney disease, CKD`），也被称为慢性肾衰竭，是指出现 `≥3` 个月的肾脏结构或功能异常、且对健康有影响的状况。这意味着肾小球滤过率低于 `60 mL/(min·1.73m²)`，或出现以下一种或多种肾损伤标志：白蛋白尿/蛋白尿、尿沉渣异常（例如血尿）、肾小管疾病引发的电解质紊乱、组织学检查发现异常、影像学检查发现结构异常或存在肾移植史。无论是急性还是慢性，所有肾脏病患者都应通过血清肌酐估算肾小球滤过率(`glomerular filtration rate, GFR`)来评估肾功能。临床上采用这种方法来评估肾脏受损程度、追踪病程及评价疗效。具体诊断过程的第一步为仔细进行尿液分析，以发现是否存在蛋白尿、血尿和细胞管型。进一步评估可能包括蛋白尿定量、肾脏超声、转诊肾病科，以及肾活检。肾功能快速减退、尿白蛋白/肌酐比值升高`(>300mg/g)`或尿液中出现红细胞管型时，尤其需要转至肾病科。



本实验旨在以慢性肾病数据为基础，使用多种机器学习算法分析慢性肾病的特征、可能的关联、类别特点、变量关系等方面的相关问题，从而深入了解慢性肾病的相关信息。



## 数据提取

|         关键属性名         |       字段含义       |                   相关解释                    |
| :------------------------: | :------------------: | :-------------------------------------------: |
|            sex             |         性别         |                  0-女  1-男                   |
|        inheritance         |   遗传性肾脏病病史   |                  无-0  有-1                   |
|           family           |    慢性肾炎家族史    |                  无-0  有-1                   |
|      renal_transplant      |      肾移植病史      |                  否-0  是-1                   |
|          puncture          |    肾穿刺活检术史    |                  否-0  是-1                   |
|        hypertension        |      高血压病史      |                  无-0  有-1                   |
|          diabetes          |      糖尿病病史      |                  无-0  有-1                   |
|       hyperuricemia        |      高尿血酸症      |                  否-0  是-1                   |
|         ultrasonic         | 肾脏超声发现构造异常 |                  无-0  有-1                   |
|   urine_routine_protein    |    尿常规蛋白指标    |                阴性-0  阳性-1                 |
|   urine_protein_positive   |      尿蛋白阳性      |    - :-1  ±:0  +:1 2+:2  3+:3  4+:4  5+:5     |
| urine_red_blood_cell_value |     尿红细胞数值     |                   正常数值                    |
|            UACR            |    尿白蛋白肌酐比    |    <30    0  \|\|  30~300  1  \|\| >300  2    |
|            CREA            |        血肌酐        |                   正常数值                    |
|            eGFR            |         eGFR         |                   正常数值                    |
|     CKD_stratification     |       CKD分层        | 低危 1 \|\| 中危 2 \|\| 高危 3 \|\| 极高危  4 |
|         CKD_rating         |       CKD评级        | 1期  1 \|\| 2期  2 \|\|  3期  3  \|\| 4期  4  |



------

## 数据预处理

### 数据清洗与数据变换

所给的数据中存在一些冗余属性和噪声，需要进行数据预处理。

1. 医院代码、医院名称、确诊日期与本实验对慢性疾病的分析并没有直接关联，直接将这两个属性删除
2. 其余的各个属性如果只有两个取值，布尔类型直接分别用0和1替换
3. 尿蛋白阳性指标中将符号换成如上的数字表示
4. 尿白蛋白肌酐比、CKD分层、CKD评级分别使用表示阶段性的数字替代



#### 处理缺失值

包含缺失值的数据在数据挖掘时会对训练得到的模型造成很坏的影响，所以首先对缺失值做处理

首先，需要查看缺失值的缺失数量以及比例

```python
# 统计缺失值
data = pd.read_excel('慢性肾病数据.xlsx')
# 统计缺失值数量
missing = data.isnull().sum().reset_index().rename(columns={0: 'missNum'})
# 计算缺失比例
missing['missRate'] = missing['missNum'] / data.shape[0]
# 按照缺失率排序显示
miss_analy = missing[missing.missRate > 0].sort_values(by='missRate', ascending=False)
print(miss_analy)
```

![image-20220104233444429](E:\MyProject\markdown\images\image-20220104233444429.png)

柱形图可视化

```python
plt.rcParams['font.sans-serif'] = ['SimHei']  # Show Chinese label
plt.rcParams['axes.unicode_minus'] = False  # These two lines need to be set manually
fig = plt.figure(figsize=(18, 10))
plt.bar(np.arange(miss_analy.shape[0]), list(miss_analy.missRate.values), align='center',
        color=['red', 'green', 'yellow', 'steelblue'])

plt.title('Histogram of missing value of variables')
plt.xlabel('variables names')
plt.ylabel('missing rate')
# 添加x轴标签，并旋转90度
plt.xticks(np.arange(miss_analy.shape[0]), list(miss_analy['index']))
pl.xticks(rotation=90)
# 添加数值显示
for x, y in enumerate(list(miss_analy.missRate.values)):
    plt.text(x, y + 0.12, '{:.2%}'.format(y), ha='center', rotation=90)
plt.ylim([0, 1.2])
plt.show()
```

![缺失值](E:\MyProject\markdown\images\缺失值.png)

从图中可以看出尿蛋白阳性属性所对应的值缺失了60%以上，这样的数据应避免在后续的数据挖掘中使用。另一个问题就是，通过数据分析可以看到有些条目的连续多个属性都缺失了，这样的数据不应该出现在后续的数据挖掘过程中，应将其删除。经过上述处理后再次查看缺失值情况

![image-20220104235645279](E:\MyProject\markdown\images\image-20220104235645279.png)

<img src="E:\MyProject\markdown\images\处理后的缺失值.png" alt="处理后的缺失值" style="zoom:200%;" />

可以看到缺失值所占的比例显著降低了

利用`knn`算法填充，把目标列当做目标标量，利用非缺失的数据进行`knn`算法拟合，最后对目标列缺失进行预测（对于连续特征一般是加权平均，对于离散特征一般是加权投票）

```python
fill_knn = KNN(k=3).fit_transform(data)
data = pd.DataFrame(fill_knn)
```

![image-20220105000943201](E:\MyProject\markdown\images\image-20220105000943201.png)



### 特征重要性筛选

剔除与结果没有影响或者影响不大的特征，有助于提高预测模型的构建速度，增强模型的泛化能力，减少过拟合问题，提升特征与特征值之间的理解。在本实验中采用逻辑回归的稳定性选择方法实现对特征的筛选。稳定性选择方法能够有效帮助筛选重要特征，同时有助于增强对数据的理解。经过前面的步骤，现在还剩下`14`个特征，从左到右分别编号为`0`至`13`，然后使用算法实现特征筛选

```python
fpath = r"慢性肾病数据.xlsx"
Dataset = pd.read_excel(fpath)

x = Dataset.loc[:, "性别":"eGFR"]
y1 = Dataset.loc[:, "CKD分层"]
y2 = Dataset.loc[:, "CKD评级"]

names = x.columns
names = list(names)
key = list(range(0, len(names)))
names_dict = dict(zip(key, names))
names_dicts = pd.DataFrame([names_dict])

x_train, x_test, y_train, y_test = train_test_split(x, y2, test_size=0.33, random_state=7)
"""
max_depth:树的最大深度
"""
model = xgb.XGBRegressor(max_depth=6, learning_rate=0.12, n_estimators=90, min_child_weight=6, objective="reg:gamma")
model.fit(x_train, y_train)
print(model.feature_importances_)

plt.rcParams['font.sans-serif'] = ['SimHei']  # Show Chinese label
plt.rcParams['axes.unicode_minus'] = False  # These two lines need to be set manually

plt.title("Xgboost Feature Importance")
plt.xlabel("Feature")
plt.ylabel("Feature Score")
plt.bar(range(len(model.feature_importances_)), model.feature_importances_)
plt.show()
```

![特征重要性筛选](E:\MyProject\markdown\images\特征重要性筛选.png)

从这个表中可以大致得出以下信息：

- 1至4特征评分基本为0，也就是说遗传性肾脏病病史、慢性肾炎家族史、肾移植病史、肾穿刺活检术史这四个特征对结果影响不大，就单从数据层面看，这四个特征所对应的值取值变化很小，所以对结果影响不大
- 12和13也就是血肌酐和eGFR特征评分最高，这两个特征对结果的影响最大



但是前面使用了`KNN`做数据填充，使用数据填充之后，再进行特征值筛选，得到这样的结果

<img src="E:\MyProject\markdown\images\KNN填充特征值筛选.png" alt="KNN填充特征值筛选" style="zoom:200%;" />

从图上看经过缺失值填充后，特征值筛选除了上述所说对应的值取值变化很小的特征筛选结果差不多外，其余的特征选取结果相差比较大，下述实验按照缺失值填充后的数据进行



------

## 慢性肾病预测建模

### 决策树分析

#### (ID3)GINI:

使用信息增益作为特征指标选择的决策树算法

分为四步：

- 计算数据集划分前的信息熵
- 遍历没有被划分的特征，计算根据每个特征划分数据集的信息熵
- 选择信息增益最大的特征作为数据划分结点来划分数据
- 递归处理被划分后的数据的子数据集，从未被选择的特征中继续划分数据

递归终止条件：

- 特征都用完了
- 信息增益已经足够小，快趋近于0了

```python
print("决策树   ID3算法------------------------------------------")
print("开始训练模型....")
dt_ID3_model = DecisionTreeClassifier(criterion="entropy")
dt_ID3_model.fit(x_train, y_train)  # 使用训练集训练模型
predict_results = dt_ID3_model.predict(x_test)  # 使用模型对测试集进行预测
print("正确率:")
print(accuracy_score(predict_results, y_test))
fig, ax = plt.subplots(figsize=(20, 16))
plot_tree(dt_ID3_model, ax=ax)
plt.savefig('决策树-ID3算法.jpg', dpi=1080)
plt.show()
```

结果：

![image-20220105143807862](E:\MyProject\markdown\images\image-20220105143807862.png)

决策树图太大，可以在根目录下查看



#### CART(entropy)

分类与回归树(`CART——Classification And Regression Tree`)通过构建二叉树达到预测目的。

算法输入训练集`D`，基尼系数的阈值，样本个数阈值。输出的是决策树`T`。算法从根节点开始，用训练集递归建立`CART`分类树。

- 对于当前节点的数据集为`D`，如果样本个数小于阈值或没有特征，则返回决策子树，当前节点停止递归
- 计算样本集`D`的基尼系数，如果基尼系数小于阈值，则返回决策树子树，当前节点停止递归
- 计算当前节点现有的各个特征的各个特征值对数据集`D`的基尼系数，缺失值的处理方法和`C4.5`算法差不多
- 在计算出来的各个特征的各个特征值对数据集`D`的基尼系数中，选择基尼系数最小的特征`A`和对应的特征值`a`。根据这个最优特征和最优特征值，把数据集划分成两部分`D1`和`D2`，同时建立当前节点的左右节点，左节点的数据集`D`为`D1`，右节点的数据集`D`为`D2`
- 对左右子节点的递归调用`1-4`步，生成决策树

对生成的决策树做预测的时候，假如测试集里的样本`A`落到了某个叶子节点，而节点里有多个训练样本，则对于`A`的类别预测采用的是这个叶子节点里概率最大的类别

```python
print("决策树   CART算法------------------------------------------")
print("开始训练模型....")
dt_CART_model = DecisionTreeClassifier()
dt_CART_model.fit(x_train, y_train)  # 使用训练集训练模型
predict_results = dt_CART_model.predict(x_test)  # 使用模型对测试集进行预测
print("正确率:")
print(accuracy_score(predict_results, y_test))
fig, ax = plt.subplots(figsize=(20, 16))
plot_tree(dt_CART_model, ax=ax)
plt.savefig('决策树-CART算法.jpg', dpi=1080)
plt.show()
```

![image-20220105144446704](E:\MyProject\markdown\images\image-20220105144446704.png)

决策树图太大，可以在根目录下查看



#### 小结

可以看到在其他条件相同的情况下，`ID3`算法的正确率略高于`CART`算法

##### ID3算法的不足

- `ID3`没考虑连续特征，比如长度，密度都是连续值，无法在`ID3`运用
- `ID3`用信息增益作为标准容易偏向取值较多的特征。然而在相同条件下，取值比较多的特征比取值少的特征信息增益大。比如一个变量有`2`个值，各为`1/2`，另一个变量为`3`个值，各为`1/3`，其实他们都是完全不确定的变量，但是取`3`个值比取`2`个值的信息增益大
- `ID3`算法没考虑缺失值问题
- 没考虑过拟合问题

##### ID3算法的改进

针对`ID3`算法`4`个主要的不足，一是不能处理连续特征，二是用信息增益作为标准容易偏向取值较多的特征，最后是缺失值处理的问题和过拟合问题

- 对不能处理连续值特征，用`C4.5`思路：将连续的特征离散化
- 对于信息增益作为标准容易偏向于取值较多特征的问题，引入一个信息增益比 `IR(Y, X)`，它是信息增益与特征熵（也称分裂信息）的比
- 对于缺失值处理的问题，主要需要解决的是两个问题，一是在样本某些特征缺失的情况下选择划分的属性，二是选定了划分属性，对于在该属性上缺失特征的样本的处理



##### CART算法的不足与改进

- 无论`ID3`，`CART`都是选择一个最优的特征做分类决策，但大多数分类决策不是由某一个特征决定，而是一组特征。这样得到的决策树更加准确，这种决策树叫多变量决策树(`multi-variate decision tree`)。在选择最优特征时，多变量决策树不是选择某一个最优特征，而是选择一个最优的特征线性组合做决策
- 样本一点点改动，树结构剧烈改变。这个通过集成学习里面的随机森林之类的方法解决



### 神经网络分析

使用`sklearn.MLPClassifier`来实现神经网络

`MLP`，`Multi-layer Perceptron`多层感知机，也叫人工神经网络（`ANN，Artificial Neural Network`），在输入输出层的中间可以有多个隐藏层，如果没有隐藏层，只能解决线性可划分的数据问题。最简单的`MLP`模型只包含一个隐藏层，即三层的结构。

多层感知机的层与层之间是全连接的（全连接的意思就是：上一层的任何一个神经元与下一层的所有神经元都有连接）。多层感知机最底层是输入层，中间是隐藏层，最后是输出层。假设输入层用向量`X`表示，则隐藏层的输出就是`f(W1X+b1)`，`W1`是权重（也叫连接系数），`b1`是偏置，函数f 可以是常用的`sigmoid`函数或者`tanh`函数。输出层的输出就是`softmax(W2X1+b2)`，`X1`表示隐藏层的输出`f(W1X+b1)`

![image-20220105160257596](E:\MyProject\markdown\images\image-20220105160257596.png)

```python
print("神经网络 ----------------------")
mlp = MLPClassifier()
mlp.fit(x_train, y_train)
print('训练集准确率=', mlp.score(x_train, y_train))
print('测试集准确率=', mlp.score(x_test, y_test))
print('训练集混淆矩阵\n', confusion_matrix(y_train, mlp.predict(x_train)))
print('测试集混淆矩阵\n', confusion_matrix(y_test, mlp.predict(x_test)))
print('训练集分类报告\n', classification_report(y_train, mlp.predict(x_train), ))
print('测试集分类报告\n', classification_report(y_test, mlp.predict(x_test)))
```

结果：

![image-20220105162355102](E:\MyProject\markdown\images\image-20220105162355102.png)

![image-20220105162422195](E:\MyProject\markdown\images\image-20220105162422195.png)

![image-20220105162435678](E:\MyProject\markdown\images\image-20220105162435678.png)

使用神经网络的主要优点：

- 获取大量数据中包含的信息
- 仔细调参后，神经网络通常能优于其他机器学习算法

缺点：

- 大型神经网络训练时间长
- 要对数据预处理

神经网络调参的主要方法是，首先创建一个大到足以过拟合的网络，直到训练数据可以被学习后，通过缩小网络或者调节正则化参数提高泛化性能。



### 关联分析

本实验采用`Apriori`算法，如果要发现强关联规则，就必须先找到频繁集。所谓频繁集，即支持度大于最小支持度的项集。

`Aprior`算法提出了一个逐层搜索的方法，包含两个步骤：

- 自连接获取候选集。第一轮的候选集就是数据集`D`中的项，而其他轮次的候选集则是由前一轮次频繁集自连接得到（频繁集由候选集剪枝得到）

- 对于候选集进行剪枝。候选集的每一条记录`T`，如果它的支持度小于最小支持度，那么就会被剪掉；此外，如果一条记录`T`，它的子集有不是频繁集的，也会被剪掉。

算法的终止条件是，如果自连接得到的已经不再是频繁集，那么取最后一次得到的频繁集作为结果。

```python
def apriori_method(data, min_support, min_confidence, min_lift, max_length):
    associate_rules = apriori(data, min_support=min_support,
                              min_confidence=min_confidence, min_lift=min_lift, max_length=max_length)

    for rule in associate_rules:
        print("频繁项集 %s，置信度 %f" % (rule.items, rule.support))
        for item in rule.ordered_statistics:
            print("%s -> %s, 置信度 %f 提升度 %f" %
                  (item.items_base, item.items_add, item.confidence, item.lift))
        print()
```

结果(部分数据)：

![image-20220105175800715](E:\MyProject\markdown\images\image-20220105175800715.png)

本实验的关联分析可作用于两个方面，首先是分析各项特征与慢性肾病的关系，另一方面是分析对于某些特征出现的时候，其他特征之间或者跟这个特征之间的关系

![image-20220105180907472](E:\MyProject\markdown\images\image-20220105180907472.png)

#### Apriori的相关特性

- 优点： 易编码实现
- 缺点： 在大数据集上可能很慢
- 适用数据类型：数值型或者标称型数据



### 回归分析

本实验采用线性回归分析

线性回归是通过一个或者多个自变量与因变量之间进行建模的回归分析

```python
lir = LinearRegression().fit(x_train, y_train)
y_pred = lir.predict(x_test)
ig = plt.figure(figsize=(10, 6))  #设定空白画布，并制定大小
plt.plot(range(y_test.shape[0]), y_test, color="blue", linewidth=1.5, linestyle="-")
plt.plot(range(y_test.shape[0]), y_pred, color="red", linewidth=1.5, linestyle="-.")
plt.legend(['真实值', '预测值'])
plt.show()
print('数据线性回归模型的平均绝对误差为：', mean_absolute_error(y_test, y_pred))
print('数据线性回归模型的均方误差为：', mean_squared_error(y_test, y_pred))
print('数据线性回归模型的中值绝对误差为：', median_absolute_error(y_test, y_pred))
print('数据线性回归模型的可解释方差值为：', explained_variance_score(y_test, y_pred))
print('数据线性回归模型的R方值为：', r2_score(y_test, y_pred))
```

结果：

![image-20220105214620212](E:\MyProject\markdown\images\image-20220105214620212.png)

![线性回归](E:\MyProject\markdown\images\线性回归.png)

线性回归的优点：

- 善于获取数据集中的线性关系
- 适用于在已有了一些预先定义好的变量并且需要一个简单的预测模型的情况下使用
- 训练速度和预测速度较快
- 在小数据集上表现很好
- 结果可解释，并且易于说明
- 当新增数据时，易于更新模型
- 不需要进行参数调整、不需要特征缩放

线性回归的缺点：

- 不适用于非线性数据
- 预测精确度较低
- 可能会出现过度拟合
- 分离信号和噪声的效果不理想，在使用前需要去掉不相关的特征
- 不了解数据集中的特征交互
- 如果数据集具有冗余的特征，那么线性回归可能是不稳定的



### 可视化分析

![image-20220105195352545](E:\MyProject\markdown\images\image-20220105195352545.png)

这是`eGFR`与`CKD`之间的关系，一般情况，`eGFR`越小，`CKD`分层和`CKD`评级越高



![image-20220105201952727](E:\MyProject\markdown\images\image-20220105201952727.png)

这是血肌酐与`CKD`的关系，可以看出，血肌酐越大，`CKD`分层与`CKD`评级有越高的趋势



![image-20220105203007818](E:\MyProject\markdown\images\image-20220105203007818.png)

可以看出患`CKD`的人有高血压病史的人更多



![image-20220105211307192](E:\MyProject\markdown\images\image-20220105211307192.png)

可以看出患`CKD`的人中没有高尿血酸症的人比有高尿血酸症的人更多



![image-20220105202848507](E:\MyProject\markdown\images\image-20220105202848507.png)

![image-20220105203917809](E:\MyProject\markdown\images\image-20220105203917809.png)



![image-20220105204301227](E:\MyProject\markdown\images\image-20220105204301227.png)

![image-20220105204338880](E:\MyProject\markdown\images\image-20220105204338880.png)

![image-20220105204423611](E:\MyProject\markdown\images\image-20220105204423611.png)



![image-20220105204514456](E:\MyProject\markdown\images\image-20220105204514456.png)

![image-20220105204631742](E:\MyProject\markdown\images\image-20220105204631742.png)

![image-20220105205438674](E:\MyProject\markdown\images\image-20220105205438674.png)

![image-20220105210043387](E:\MyProject\markdown\images\image-20220105210043387.png)

从上面几幅图中可以看出`CKD`与糖尿病病史、肾脏超声发现构造异常、慢性肾炎家族史、肾穿刺活检术史、肾移植病史、遗传性肾脏病病史关系都不大，因为这份数据反应的是有`CKD`的人是否有这些特征，但是这些病史或者特征对于人群来说本身就是小概率事件，至少从这份数据来看是反映不出这些特征是否与`CKD`有强烈的关系，`CKD`与性别、尿白蛋白肌酐比、尿常规蛋白指标关系也不大



![image-20220105210725896](E:\MyProject\markdown\images\image-20220105210725896.png)

从上图可以看出患`CKD`的人大多数尿红细胞数值都比较小(并不代表不可以作为判断`CKD`的一项指标，这只是反应尿红细胞多少的问题，而不能反应尿红细胞有无与`CKD`关联的问题)



------

## 总结

通过决策树，关联分析、可视化分析等可以大致得出这样的结果：

1. 一般而言`eGFR`越小，`CKD`分层和`CKD`评级就越高
2. 血肌酐越大，有`CKD`分层与`CKD`评级越高的趋势
3. 患`CKD`的人有高血压病史的人数更多
4. 至少从所给数据，其他特征与`CKD`关系并不大
5. 总的来说，所给数据是基于患了`CKD`再看其他特征，所以从数据中看有的特征并不能反应是不是能够帮助确诊`CDK`，而应该说`CDK`确诊后病患的其他指标是个什么样子，从而反推什么样特征的人更容易患`CKD`
