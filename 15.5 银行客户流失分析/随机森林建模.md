# 随机森林建模

## 简介

随机森林是由一堆决策树组成的，每一个决策树有一个结果，看有多少个决策树对同一个`Y`进行投票来确定`Y`。**分类就是少数服从多数，回归就是各个决策树取平均值**。随机森林是平均多个深决策树以降低方差的一种方法，其中，决策树是在一个数据集上的不同部分进行训练的。这是以偏差的小幅增加和一些可解释性的丧失为代价的，但是在最终的模型中通常会大大提高性能。



## 建模过程

1. 用有抽样放回的方法（`bootstrap`）从样本集中选取n个样本作为一个训练集
2. 用抽样得到的样本集生成一棵决策树。在生成的每一个结点随机不重复地选择`d`个特征，利用这`d`个特征分别对样本集进行划分，找到最佳的划分特征（可用基尼系数、增益率或者信息增益判别）
3. 重复步骤`1`到步骤`2`共`k`次，`k`即为随机森林中决策树的个数
4. 用训练得到的随机森林对测试样本进行预测，并用票选法决定预测的结果



![image-20211101135115355](E:\MyProject\markdown\images\image-20211101135115355.png)



### 实例分析

![image-20211101140251039](E:\MyProject\markdown\images\image-20211101140251039.png)

这是银行客户流失数据的随机森林，使用了随机森林分类算法，其所有基评估器都是决策树

#### `n_estimators`:

这是森林中树木的数量，即基评估器的数量。这个参数对随机森林模型的精确性影响是单调的，**`n_estimators`越大，模型的效果往往越好**。但是相应的，任何模型都有决策边界，`n_estimators`达到一定的程度之后，随机森林的精确性往往不再上升或开始波动，并且，`n_estimators`越大，需要的计算量和内存也越大，训练的时间也会越来越长。对于这个参数，我们是渴望在训练难度和模型效果之间取得平衡。

#### `max_depth`:

树的最大深度，超过最大深度的树枝都会被剪掉

#### `criterion`:

不纯度的衡量指标，有基尼系数和信息熵两种选择

#### `random_state`:

对于随机森林这个模型，它本质上是随机的，设置不同的随机状态（或者不设置`random_state`参数）可以彻底改变构建的模型，固定`random_state`后，每次构建的模型是相同的、生成的数据集是相同的、每次的拆分结果也是相同的

#### 改变参数分析性能变化

1. 改变`n_estimators`，观察`accuracy_score`变化

   ![image-20211101143338974](E:\MyProject\markdown\images\image-20211101143338974.png)

   ![image-20211101143445668](E:\MyProject\markdown\images\image-20211101143445668.png)

   ![image-20211101142853734](E:\MyProject\markdown\images\image-20211101142853734.png)

   ![image-20211101142947581](E:\MyProject\markdown\images\image-20211101142947581.png)

   ![image-20211101143105066](E:\MyProject\markdown\images\image-20211101143105066.png)

   ![image-20211101143240717](E:\MyProject\markdown\images\image-20211101143240717.png)

   可以发现，随着`n_estimators`从`8`增加到`13`，`accuracy_score`先增大，之后就基本不变了，说明`n_estimators`增大，一定范围内可以改善`accuracy_score`，达到一定程度后`accuracy_score`就基本不变了，达到了决策边界

2. 改变`max_depth`，观察`accuracy_score`变化

   ![image-20211101144121947](E:\MyProject\markdown\images\image-20211101144121947.png)

   ![image-20211101144209623](E:\MyProject\markdown\images\image-20211101144209623.png)

   ![image-20211101144252496](E:\MyProject\markdown\images\image-20211101144252496.png)

   ![image-20211101144331376](E:\MyProject\markdown\images\image-20211101144331376.png)

   ![image-20211101144413124](E:\MyProject\markdown\images\image-20211101144413124.png)

   ![image-20211101144505412](E:\MyProject\markdown\images\image-20211101144505412.png)

   可以看出，随着`max_depth`从`6`增加到`11`，`accuracy_score`一直在增加，说明`max_depth`增大，`accuracy_score`也在变大

3. 改变`criterion`，观察`accuracy_score`变化

   ![image-20211101145102398](E:\MyProject\markdown\images\image-20211101145102398.png)

   ![image-20211101145203284](E:\MyProject\markdown\images\image-20211101145203284.png)

   ![image-20211101145355108](E:\MyProject\markdown\images\image-20211101145355108.png)

   ![image-20211101145251977](E:\MyProject\markdown\images\image-20211101145251977.png)

   ![image-20211101145451871](E:\MyProject\markdown\images\image-20211101145451871.png)

   ![image-20211101145531940](E:\MyProject\markdown\images\image-20211101145531940.png)

   通过改变`criterion`，分别改变`n_estimators`、`max_depth`，发现在相同条件下，`gini`比`entropy`的`accuracy_score`更大

   

